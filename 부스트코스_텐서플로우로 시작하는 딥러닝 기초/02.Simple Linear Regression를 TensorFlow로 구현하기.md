## Simple Linear Regression를 TensorFlow로 구현하기

**Hypothesis(가설)**	H(x) = Wx + b	가상의 일직선

```python
x_data = [1, 2, 3, 4, 5]
y_data = [1, 2, 3, 4, 5]

W = tf.Variable(2.9)
b = tf.Variable(0.5)

# hypothesis = W * x + b
hypothesis = W * x_data + b
```

**Cost(비용함수)** 	에러(데이터와 가설 사이의 차)의 제곱의 평균

```python
cost = tf.reduce_mean(tf.square(hypothesis - y_data))
```

* tf.reduce_mean()

  ```python
  v = [1, 2, 3, 4]
  tf.reduce_mean(v)	# 2.5
  ```

* tf.square()

  ```python
  tf.square(3)	# 9
  ```



**Gradient descent(경사 하강 알고리즘)** - *minimize cost(W, b)*

```python
# Learning_rate initialize
learning_rate = 0.01	# 보통 0.0001, 0.00001 가능한 작게

# Gradient descent
with tf.GradientTape() as tape:	# with 구문 안에 있는 변수들의 변화를 tape에 기록
    hypothesis = W * x_data + b
    cost = tf.reduce_mean(tf.square(hypothesis - y_data))
    
W_grad, b_grad = tape.gradient(cost, [W, b])	# cost에 대한 [W,b]의 개별 미분값 튜플 형식으로 반환 (gradient=기울기)

W.assign_sub(learning_rate * W_grad)
b.assign_sub(learning_rate * b_grad)

'''
A.assign_sub(B) ==> A -= B
'''
```

**Parameter(W,b) Update**

```python
W = tf.Variable(2.9)
b = tf.Variable(0.5)

for i in range(100):	# W, b 변수 100번 갱신
    # Gradient descent
    with tf.GradientTape() as tape:	
        hypothesis = W * x_data + b
        cost = tf.reduce_mean(tf.square(hypothesis - y_data))

    W_grad, b_grad = tape.gradient(cost, [W, b])	

    W.assign_sub(learning_rate * W_grad)
    b.assign_sub(learning_rate * b_grad)
    
    if i%10==0:
        print('{:5}|{:10.4}|{:10.4}|{:10.6f}'.format(i, W.numpy(), b.numpy(), cost))
```

**Full Code**

```python
import tensorflow as tf
tf.enable_eager_execution()		# 즉시 실행하도록

# Data
x_data = [1, 2, 3, 4, 5]
y_data = [1, 2, 3, 4, 5]

# W, b initialize
W = tf.Variable(2.9)
b = tf.Variable(0.5)

learning_rate = 0.01

for i in range(100):	# W, b update
    # Gradient descent
    with tf.GradientTape() as tape:	
        hypothesis = W * x_data + b
        cost = tf.reduce_mean(tf.square(hypothesis - y_data))
	W_grad, b_grad = tape.gradient(cost, [W, b])	
    W.assign_sub(learning_rate * W_grad)
    b.assign_sub(learning_rate * b_grad)
    if i%10==0:
        print('{:5}|{:10.4}|{:10.4}|{:10.6f}'.format(i, W.numpy(), b.numpy(), cost))
```

**Predict**

```python
print(W * 5 + b)	# 5.0066934
print(W * 2.5 + b)	# 2.4946523
```



