## Linear Regression and How to minimize cost

**Gradient descent algorithm**

* Minimize cost function
* Gradient descent is used many minimization problems
* For a given cost function, cost(W, b), it will find W , b to minimize cost
* It can be applied to more general function: cost(w1, w2, ...)



**How it works?**

* Start with initial guesses
  * Start at 0, 0 (or any other value)
  * Keeping changing W and b a little bit to try and reduce cost(W, b)
* Each time you change the parameters, you select the gradient which reduces cost(W, b) the most possible
* Repeat
* Do so until you converge to a local minimum
* Has an interesting property
  * Where you can start can determine which minimum you end up



**Gradient descent algorithm**

![$$ W:=W-\alpha \frac { 1 }{ m } \sum _{ i=1 }^{ m }{ { (W{ x }^{ i }-y^{ i } })x^{ i } }  $$](https://render.githubusercontent.com/render/math?math=W%3A%3DW-%5Calpha%20%5Cfrac%20%7B%201%20%7D%7B%20m%20%7D%20%5Csum%20_%7B%20i%3D1%20%7D%5E%7B%20m%20%7D%7B%20%7B%20%28W%7B%20x%20%7D%5E%7B%20i%20%7D-y%5E%7B%20i%20%7D%20%7D%29x%5E%7B%20i%20%7D%20%7D&mode=display)

α : learning-rate

```python
alpha = 0.01
gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, x) - Y, X))
descent = W - tf.multiply(alpha, gradient)
W.assign(descent)
```



**Cost function in pure Python**

```python
import numpy as np

X = np.array([1, 2, 3])
Y = np.array([1, 2, 3])

def cost_func(W, X, Y):
    c = 0
    for i in range(len(X)):
        c += (W * X[i] - Y[i]) ** 2
    return c / len(X)

for feed_W in np.linspace(-3, 5, num=15):
    curr_cost = cost_func(feed_W, X, Y)
    print("{:6.3f} | {:10.5f}".format(feed_W, curr_cost))
    
'''
-3.000 |   74.66667
-2.429 |   54.85714
-1.857 |   38.09524
-1.286 |   24.38095
-0.714 |   13.71429
-0.143 |    6.09524
 0.429 |    1.52381
 1.000 |    0.00000
 1.571 |    1.52381
 2.143 |    6.09524
 2.714 |   13.71429
 3.286 |   24.38095
 3.857 |   38.09524
 4.429 |   54.85714
 5.000 |   74.66667
 '''
```



**Cost function in TensorFlow**

```python

X = np.array([1, 2, 3])
Y = np.array([1, 2, 3])

def cost_func(W, X, Y):
  hypothesis = X * W
  return tf.reduce_mean(tf.square(hypothesis - Y))

W_values = np.linspace(-3, 5, num=15)
cost_values = []

for feed_W in W_values:
    curr_cost = cost_func(feed_W, X, Y)
    cost_values.append(curr_cost)
    print("{:6.3f} | {:10.5f}".format(feed_W, curr_cost))
    
'''
-3.000 |   74.66667
-2.429 |   54.85714
-1.857 |   38.09524
-1.286 |   24.38095
-0.714 |   13.71429
-0.143 |    6.09524
 0.429 |    1.52381
 1.000 |    0.00000
 1.571 |    1.52381
 2.143 |    6.09524
 2.714 |   13.71429
 3.286 |   24.38095
 3.857 |   38.09524
 4.429 |   54.85714
 5.000 |   74.66667
 '''
```



**Gradient descent**

```python
tf.random.set_seed(0)  # for reproducibility 이 코드를 다시 수행했을 때도 동일하게 똑같이 진행이 될 수 있도록 하기 위해서 랜덤 시드를 특정한 값으로 초기화

x_data = [1., 2., 3., 4.]
y_data = [1., 3., 5., 7.]

W = tf.Variable(tf.random.normal([1], -100., 100.))

for step in range(300):
    hypothesis = W * x_data
    cost = tf.reduce_mean(tf.square(hypothesis - y_data))

    alpha = 0.01
    gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, x_data) - y_data, x_data))
    descent = W - tf.multiply(alpha, gradient)
    W.assign(descent)
    
    if step % 10 == 0:
        print('{:5} | {:10.4f} | {:10.6f}'.format(
            step, cost.numpy(), W.numpy()[0]))
        
'''
    0 | 18332.2188 |  47.398293
   10 |  3855.3564 |  22.638384
   20 |   810.9046 |  11.283927
   30 |   170.6631 |   6.076973
   40 |    36.0217 |   3.689155
   50 |     7.7069 |   2.594144
   60 |     1.7524 |   2.091991
   70 |     0.5001 |   1.861713
   80 |     0.2368 |   1.756112
   90 |     0.1814 |   1.707684
  100 |     0.1698 |   1.685477
  110 |     0.1673 |   1.675292
  120 |     0.1668 |   1.670622
  130 |     0.1667 |   1.668481
  140 |     0.1667 |   1.667498
  150 |     0.1667 |   1.667048
  160 |     0.1667 |   1.666842
  170 |     0.1667 |   1.666747
  180 |     0.1667 |   1.666703
  190 |     0.1667 |   1.666684
  200 |     0.1667 |   1.666674
  210 |     0.1667 |   1.666670
  220 |     0.1667 |   1.666668
  230 |     0.1667 |   1.666667
  240 |     0.1667 |   1.666667
  250 |     0.1667 |   1.666667
  260 |     0.1667 |   1.666667
  270 |     0.1667 |   1.666667
  280 |     0.1667 |   1.666667
  290 |     0.1667 |   1.666667
  '''
```

