# 붓꽃 품종 분류기

> KNN 알고리즘(K - Nearest Neighbors)
>
> 여러 학습 데이터로 학습시킨 뒤 새로운 데이터를 입력했을 때 그 데이터와 가장 가까운 값 K개를 찾고, 가장 빈도가 높은 레이블을 결과 레이블로 내놓는다.

```python
# KNN(K Nearest Neighbors): K-최근접 이웃 알고리즘
# 사용하기 쉬운 분류 알고리즘(분류기) 중의 하나이다.

# K의 의미는 가장 가까운 이웃 하나를 의미하는 것이 아니라,
# 훈련데이터에서 새로운 데이터에 가장 가까운 K개의 이웃을 찾는다는 의미

# KNN을 사용하기 위해서는 neightbors모듈에 KNeighborsClassifier 함수 사용
# KNeighborsClassifier() 함수 중에 중요한 매개변수는 n_neighbors
# 이 매개변수는 이웃의 개수를 지정하는 매개변수이다.
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n-neighbors=1)

# 훈련 데이터셋을 가지고 모델을 만드려면 fit 메서드 사용
# fit 메서드의 리턴 값은 knn객체를 리턴한다.
knn.fit(X_train, y_train)

# 채집한 붓꽃의 새로운 데이터(샘플)라고 가정하고 Numpy배열로 특성값을 만든다.
# scikit-learn에서는 항상 데이터가 2차원 배열일 것으로 예측한다.

X_newData = np.array([[5.1, 2.9, 1, 0.3]])

# knn 객체의 predict()메서드를 사용하여 예측을 할 수 있다.
prediction = knn.predict(X_newData)		# => 0 (클래스명 추출 0: setosa)
print("예측: {}".format(prediction))
print("예측 품종의 이름: {}".format(irisData['target_names'][prediction]))

y_predict = knn.predict(X_test)

# 정확도를 계산하기 위해서 numpy의 mean()메서드를 이용
# knn객체의 score()메서드를 사용해도 된다.
np.mean(y_predict==y_test)		#=> 0.9736842....
print("정확도: {}:.2f".format(np.mean(y_predict==y_test)))

knn.score(X_test, y_test)
print("정확도: {}:.2f".format(knn.score(X_test, y_test)))

# 머신러닝의 용어정리
# iris 분류 문제에 있어서 각 품종을 클래스라고 한다.
# 개별 붓꽃의 품종은 레이블이라고 한다.

# 붓꽃의 데이터 셋은 두 개의 numpy 배열로 이루어져 있다.
# 하나는 데이터, 다른 하나는 출력을 가지고 있다.
# scikit-learn에서는 데이터는 X로 표기하고, 출력은 y로 표기한다.

# 이 때 배열 X는 2차원 배열이고 각 행은 데이터포인트(샘플)에 해당한다.
# 각 컬럼(열)은 특성이라고 한다.
# 배열 y는 1차원 배열이고, 각 샘플의 클래스 레이블에 해당한다.


```

**파일 저장**

https://github.com/pandas-dev/pandas/blob/master/pandas/tests/data/iris.csv

```python
from sklearn import svm, metrics
import random, re	# Regular Expression 정규표현식

# 붓꽃의 데이터를 읽어 들이기
csv = []
with open('iris.csv', 'r', encoding='utf=8') as fp:
    # 한 줄씩 읽어오기
    for line in fp:
        line = line.strip()		# 줄바꿈 제거
        cols = line.split(',')	# 쉼표로 컬럼을 구분
        # 문자열 데이터를 숫자로 변환하기
        fn = lambda n: float(n) if re.match(r'^[0-9\.]+$', n) else n
        cols = list(map(fn, cols))
        csv.append(cols)
        
# 헤더 제거(컬럼명 제거)
del csv[0]

# 데이터를 섞어주기
random.shuffle(csv)

# 훈련(학습)데이터와 테스트 데이터로 분리하기
total_len = len(csv)
train_len = int(total_len*2/3)

train_data = []
train_label = []
test_data = []
test_label = []

for i in range(total_len):
    data = csv[i][0:4]
    label = csv[i][4]
    if i < train_len:
        train_data.append(data)
        train_label.append(label)
    else:
        test_data.append(data)
        test_label.append(label)
        
clf = svm.SVC()
# 학습
clf.fit(train_data, train_label)

# 테스트
pre_label = clf.predict(test_data)

# 정확도
ac_score = metrics.accuracy_score(test_label, pre_label)

print("정확도: ", ac_score)
        
```

**pandas 이용**

```python
import pandas as pd
from sklearn import svm, metrics
from sklearn.model_selection import train_test_split

# 데이터 읽어오기(pandas 이용)
csv = pd.read_csv('iris.csv')

# 데이터와 레이블 분리하기
csv_data = csv[['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']]
csv_label = csv['Name']

# 훈련 데이터와 테스트 데이터로 분리하기
X_train, X_test, y_train, y_test = train_test_split(csv_data, csv_label)

clf = svm.SVC()
clf.fit(X_train, y_train)
y_predict = clf.predict(X_test)

ac_score = metrics.accuracy_score(y_test, y_predict)
print('정확도: ', ac_score)

```

