{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0|     2.452|     0.376| 45.660004\n",
      "   10|     1.104|  0.003398|  0.206336\n",
      "   20|     1.013|  -0.02091|  0.001026\n",
      "   30|     1.007|  -0.02184|  0.000093\n",
      "   40|     1.006|  -0.02123|  0.000083\n",
      "   50|     1.006|  -0.02053|  0.000077\n",
      "   60|     1.005|  -0.01984|  0.000072\n",
      "   70|     1.005|  -0.01918|  0.000067\n",
      "   80|     1.005|  -0.01854|  0.000063\n",
      "   90|     1.005|  -0.01793|  0.000059\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution()\t\t# 즉시 실행하도록\n",
    "\n",
    "# Data\n",
    "x_data = [1, 2, 3, 4, 5]\n",
    "y_data = [1, 2, 3, 4, 5]\n",
    "\n",
    "# W, b initialize\n",
    "W = tf.Variable(2.9)\n",
    "b = tf.Variable(0.5)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i in range(100):\t# W, b update\n",
    "    # Gradient descent\n",
    "    with tf.GradientTape() as tape:\t\n",
    "        hypothesis = W * x_data + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    if i%10==0:\n",
    "        print('{:5}|{:10.4}|{:10.4}|{:10.6f}'.format(i, W.numpy(), b.numpy(), cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(5.0066934, shape=(), dtype=float32)\n",
      "tf.Tensor(2.4946523, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(W * 5 + b)\t# 5.0066934\n",
    "print(W * 2.5 + b)\t# 2.4946523"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.000 |   74.66667\n",
      "-2.429 |   54.85714\n",
      "-1.857 |   38.09524\n",
      "-1.286 |   24.38095\n",
      "-0.714 |   13.71429\n",
      "-0.143 |    6.09524\n",
      " 0.429 |    1.52381\n",
      " 1.000 |    0.00000\n",
      " 1.571 |    1.52381\n",
      " 2.143 |    6.09524\n",
      " 2.714 |   13.71429\n",
      " 3.286 |   24.38095\n",
      " 3.857 |   38.09524\n",
      " 4.429 |   54.85714\n",
      " 5.000 |   74.66667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([1, 2, 3])\n",
    "Y = np.array([1, 2, 3])\n",
    "\n",
    "def cost_func(W, X, Y):\n",
    "    c = 0\n",
    "    for i in range(len(X)):\n",
    "        c += (W * X[i] - Y[i]) ** 2\n",
    "    return c / len(X)\n",
    "\n",
    "for feed_W in np.linspace(-3, 5, num=15):\n",
    "    curr_cost = cost_func(feed_W, X, Y)\n",
    "    print(\"{:6.3f} | {:10.5f}\".format(feed_W, curr_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.000 |   74.66667\n",
      "-2.429 |   54.85714\n",
      "-1.857 |   38.09524\n",
      "-1.286 |   24.38095\n",
      "-0.714 |   13.71429\n",
      "-0.143 |    6.09524\n",
      " 0.429 |    1.52381\n",
      " 1.000 |    0.00000\n",
      " 1.571 |    1.52381\n",
      " 2.143 |    6.09524\n",
      " 2.714 |   13.71429\n",
      " 3.286 |   24.38095\n",
      " 3.857 |   38.09524\n",
      " 4.429 |   54.85714\n",
      " 5.000 |   74.66667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = np.array([1, 2, 3])\n",
    "Y = np.array([1, 2, 3])\n",
    "\n",
    "def cost_func(W, X, Y):\n",
    "  hypothesis = X * W\n",
    "  return tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "W_values = np.linspace(-3, 5, num=15)\n",
    "cost_values = []\n",
    "\n",
    "for feed_W in W_values:\n",
    "    curr_cost = cost_func(feed_W, X, Y)\n",
    "    cost_values.append(curr_cost)\n",
    "    print(\"{:6.3f} | {:10.5f}\".format(feed_W, curr_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 | 18332.2188 |  47.398293\n",
      "   10 |  3855.3564 |  22.638384\n",
      "   20 |   810.9046 |  11.283927\n",
      "   30 |   170.6631 |   6.076973\n",
      "   40 |    36.0217 |   3.689155\n",
      "   50 |     7.7069 |   2.594144\n",
      "   60 |     1.7524 |   2.091991\n",
      "   70 |     0.5001 |   1.861713\n",
      "   80 |     0.2368 |   1.756112\n",
      "   90 |     0.1814 |   1.707684\n",
      "  100 |     0.1698 |   1.685477\n",
      "  110 |     0.1673 |   1.675292\n",
      "  120 |     0.1668 |   1.670622\n",
      "  130 |     0.1667 |   1.668481\n",
      "  140 |     0.1667 |   1.667498\n",
      "  150 |     0.1667 |   1.667048\n",
      "  160 |     0.1667 |   1.666842\n",
      "  170 |     0.1667 |   1.666747\n",
      "  180 |     0.1667 |   1.666703\n",
      "  190 |     0.1667 |   1.666684\n",
      "  200 |     0.1667 |   1.666674\n",
      "  210 |     0.1667 |   1.666670\n",
      "  220 |     0.1667 |   1.666668\n",
      "  230 |     0.1667 |   1.666667\n",
      "  240 |     0.1667 |   1.666667\n",
      "  250 |     0.1667 |   1.666667\n",
      "  260 |     0.1667 |   1.666667\n",
      "  270 |     0.1667 |   1.666667\n",
      "  280 |     0.1667 |   1.666667\n",
      "  290 |     0.1667 |   1.666667\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)  # for reproducibility 이 코드를 다시 수행했을 때도 동일하게 똑같이 진행이 될 수 있도록 하기 위해서 랜덤 시드를 특정한 값으로 초기화\n",
    "\n",
    "x_data = [1., 2., 3., 4.]\n",
    "y_data = [1., 3., 5., 7.]\n",
    "\n",
    "W = tf.Variable(tf.random.normal([1], -100., 100.))\n",
    "\n",
    "for step in range(300):\n",
    "    hypothesis = W * x_data\n",
    "    cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "    alpha = 0.01\n",
    "    gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, x_data) - y_data, x_data))\n",
    "    descent = W - tf.multiply(alpha, gradient)\n",
    "    W.assign(descent)\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print('{:5} | {:10.4f} | {:10.6f}'.format(\n",
    "            step, cost.numpy(), W.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |    7575.0693\n",
      "   50 |     140.8121\n",
      "  100 |      58.1745\n",
      "  150 |      57.1111\n",
      "  200 |      56.9531\n",
      "  250 |      56.8058\n",
      "  300 |      56.6587\n",
      "  350 |      56.5122\n",
      "  400 |      56.3662\n",
      "  450 |      56.2205\n",
      "  500 |      56.0753\n",
      "  550 |      55.9304\n",
      "  600 |      55.7858\n",
      "  650 |      55.6417\n",
      "  700 |      55.4979\n",
      "  750 |      55.3544\n",
      "  800 |      55.2114\n",
      "  850 |      55.0688\n",
      "  900 |      54.9265\n",
      "  950 |      54.7847\n",
      " 1000 |      54.6432\n"
     ]
    }
   ],
   "source": [
    "# data and label\n",
    "x1 = [ 73.,  93.,  89.,  96.,  73.]\n",
    "x2 = [ 80.,  88.,  91.,  98.,  66.]\n",
    "x3 = [ 75.,  93.,  90., 100.,  70.]\n",
    "Y  = [152., 185., 180., 196., 142.]\n",
    "\n",
    "# random weights\n",
    "w1 = tf.Variable(tf.random.normal([1]))\n",
    "w2 = tf.Variable(tf.random.normal([1]))\n",
    "w3 = tf.Variable(tf.random.normal([1]))\n",
    "b  = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "learning_rate = 0.000001\n",
    "\n",
    "for i in range(1000+1):\n",
    "    # tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "    # calculates the gradients of the cost\n",
    "    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
    "    \n",
    "    # update w1,w2,w3 and b\n",
    "    w1.assign_sub(learning_rate * w1_grad)\n",
    "    w2.assign_sub(learning_rate * w2_grad)\n",
    "    w3.assign_sub(learning_rate * w3_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "      print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch | cost\n",
      "    0 |  4567.1289\n",
      "  100 |     2.3131\n",
      "  200 |     1.7445\n",
      "  300 |     1.7378\n",
      "  400 |     1.7313\n",
      "  500 |     1.7249\n",
      "  600 |     1.7184\n",
      "  700 |     1.7121\n",
      "  800 |     1.7057\n",
      "  900 |     1.6994\n",
      " 1000 |     1.6931\n",
      " 1100 |     1.6868\n",
      " 1200 |     1.6805\n",
      " 1300 |     1.6743\n",
      " 1400 |     1.6681\n",
      " 1500 |     1.6621\n",
      " 1600 |     1.6560\n",
      " 1700 |     1.6499\n",
      " 1800 |     1.6438\n",
      " 1900 |     1.6378\n",
      " 2000 |     1.6319\n"
     ]
    }
   ],
   "source": [
    "data = np.array([\n",
    "    # X1,   X2,    X3,   y\n",
    "    [ 73.,  80.,  75., 152. ],\n",
    "    [ 93.,  88.,  93., 185. ],\n",
    "    [ 89.,  91.,  90., 180. ],\n",
    "    [ 96.,  98., 100., 196. ],\n",
    "    [ 73.,  66.,  70., 142. ]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# slice data\n",
    "X = data[:, :-1]\t# data 3열까지(입력)\n",
    "y = data[:, [-1]]\t# data 4열(출력)\n",
    "\n",
    "W = tf.Variable(tf.random.normal([3, 1]))\t# 3행 1열\n",
    "b = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "learning_rate = 0.000001\n",
    "\n",
    "# hypothesis, prediction function\n",
    "def predict(X):\n",
    "    return tf.matmul(X, W) + b\t# matmul => matrix multiply(행렬곱)\n",
    "\n",
    "print(\"epoch | cost\")\n",
    "\n",
    "n_epochs = 2000\n",
    "for i in range(n_epochs+1):\n",
    "    # tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = tf.reduce_mean((tf.square(predict(X) - y)))\n",
    "\n",
    "    # calculates the gradients of the loss\n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "\n",
    "    # updates parameters (W and b)\n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"{:5} | {:10.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 0.6874\n",
      "Iter: 100, Loss: 0.5776\n",
      "Iter: 200, Loss: 0.5349\n",
      "Iter: 300, Loss: 0.5054\n",
      "Iter: 400, Loss: 0.4838\n",
      "Iter: 500, Loss: 0.4671\n",
      "Iter: 600, Loss: 0.4535\n",
      "Iter: 700, Loss: 0.4420\n",
      "Iter: 800, Loss: 0.4319\n",
      "Iter: 900, Loss: 0.4228\n",
      "Iter: 1000, Loss: 0.4144\n",
      "Testset Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x_train = [[1., 2.],\n",
    "          [2., 3.],\n",
    "          [3., 1.],\n",
    "          [4., 3.],\n",
    "          [5., 3.],\n",
    "          [6., 2.]]\n",
    "y_train = [[0.],\n",
    "          [0.],\n",
    "          [0.],\n",
    "          [1.],\n",
    "          [1.],\n",
    "          [1.]]\n",
    "\n",
    "x_test = [[5.,2.]]\t#데이터셋 설정\n",
    "y_test = [[1.]]\n",
    "\n",
    "# Batch Size는 한 번에 학습시킬 Size로 정한다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))\n",
    "W = tf.Variable(tf.zeros([2,1]), name='weight')\n",
    "b = tf.Variable(tf.zeros([1]), name='bias')\t# W, b 초기설정\n",
    "\n",
    "def logistic_regression(features):\t# Sigmoid 함수를 가설로 선언\n",
    "    hypothesis  = tf.divide(1., 1. + tf.exp(tf.matmul(features, W) + b))\n",
    "    return hypothesis\n",
    "\n",
    "def loss_fn(hypothesis, features, labels):\t# 가설을 검증할 Cost 함수 정의\n",
    "    cost = -tf.reduce_mean(labels * tf.math.log(logistic_regression(features)) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
    "    return cost\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "def accuracy_fn(hypothesis, labels):\t# 예측값이 0.5보다 크면 1 반환, 아니면 0 반환\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))\n",
    "    return accuracy\n",
    "\n",
    "def grad(features, labels):\t# 경사값 계산\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(logistic_regression(features),features,labels)\n",
    "    return tape.gradient(loss_value, [W,b])\n",
    "\n",
    "EPOCHS = 1001\n",
    "\n",
    "for step in range(EPOCHS):\n",
    "    for features, labels  in iter(dataset):\n",
    "        grads = grad(features, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))\n",
    "        if step % 100 == 0:\n",
    "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(logistic_regression(features),features,labels)))\n",
    "test_acc = accuracy_fn(logistic_regression(x_test),y_test)\n",
    "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "#convert into numpy and float format\n",
    "x_data = np.asarray(x_data, dtype=np.float32)\n",
    "y_data = np.asarray(y_data, dtype=np.float32)\n",
    "\n",
    "nb_classes = 3\t# num classes (A, B, C - ONE-HOT-ENCODING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'weight:0' shape=(4, 3) dtype=float32, numpy=\n",
      "array([[-0.6701146 , -1.5312574 ,  0.73224336],\n",
      "       [-0.37725016, -0.62765795, -2.5822446 ],\n",
      "       [-0.67284334,  0.25653654, -0.3872052 ],\n",
      "       [-0.040566  , -1.2135769 , -0.74410707]], dtype=float32)> <tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([ 0.83098805,  1.1333642 , -2.021677  ], dtype=float32)>\n",
      "tf.Tensor(\n",
      "[[7.8521639e-01 2.1330991e-01 1.4736763e-03]\n",
      " [7.3885781e-01 2.1635184e-01 4.4790406e-02]\n",
      " [9.3267387e-01 1.1052889e-02 5.6273289e-02]\n",
      " [8.1645322e-01 8.1190309e-03 1.7542771e-01]\n",
      " [9.7152317e-01 2.8476795e-02 5.5822547e-09]\n",
      " [9.6908945e-01 3.0741360e-02 1.6914570e-04]\n",
      " [9.7133505e-01 2.8664880e-02 3.3331048e-08]\n",
      " [9.8231226e-01 1.7687758e-02 2.4469222e-09]], shape=(8, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Weight and bias setting\n",
    "W = tf.Variable(tf.random.normal([4, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([nb_classes]), name='bias')\n",
    "variables = [W, b]\n",
    "\n",
    "print(W,b)\n",
    "\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "print(hypothesis(x_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1.9345909e-01 3.7530765e-06 8.0653715e-01]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sample_db = [[8,2,1,4]]\n",
    "sample_db = np.asarray(sample_db, dtype=np.float32)\n",
    "\n",
    "print(hypothesis(sample_db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3.0505724, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def cost_fn(X, Y):\n",
    "    logits = hypothesis(X)\n",
    "    cost = -tf.reduce_sum(Y * tf.math.log(logits), axis=1)\n",
    "    cost_mean = tf.reduce_mean(cost)\n",
    "    \n",
    "    return cost_mean\n",
    "\n",
    "print(cost_fn(x_data, y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[ 1.2776283 , -0.6478476 , -0.6297807 ],\n",
      "       [ 1.5626819 , -1.097654  , -0.46502787],\n",
      "       [ 2.4111671 , -1.6839983 , -0.72716886],\n",
      "       [ 2.5565305 , -1.8308179 , -0.7257124 ]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 0.6459326 , -0.30569944, -0.34023324], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "def grad_fn(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = cost_fn(X, Y)\n",
    "        grads = tape.gradient(cost, variables)\n",
    "        return grads\n",
    "    \n",
    "print(grad_fn(x_data, y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1: 2.366735\n",
      "Loss at epoch 100: 0.828323\n",
      "Loss at epoch 200: 0.713423\n",
      "Loss at epoch 300: 0.645316\n",
      "Loss at epoch 400: 0.590446\n",
      "Loss at epoch 500: 0.540575\n",
      "Loss at epoch 600: 0.493011\n",
      "Loss at epoch 700: 0.446594\n",
      "Loss at epoch 800: 0.400709\n",
      "Loss at epoch 900: 0.355054\n",
      "Loss at epoch 1000: 0.309804\n",
      "Loss at epoch 1100: 0.267477\n",
      "Loss at epoch 1200: 0.242524\n",
      "Loss at epoch 1300: 0.230401\n",
      "Loss at epoch 1400: 0.219474\n",
      "Loss at epoch 1500: 0.209489\n",
      "Loss at epoch 1600: 0.200331\n",
      "Loss at epoch 1700: 0.191903\n",
      "Loss at epoch 1800: 0.184123\n",
      "Loss at epoch 1900: 0.176922\n",
      "Loss at epoch 2000: 0.170238\n"
     ]
    }
   ],
   "source": [
    "def fit(X, Y, epochs=2000, verbose=100):\n",
    "    optimizer = tf.optimizers.SGD(learning_rate=0.1, name='SGD')\n",
    "    for i in range(epochs):\n",
    "        grads = grad_fn(X, Y)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        if (i==0) | ((i+1)%verbose==0):\n",
    "            print('Loss at epoch %d: %f' %(i+1, cost_fn(X, Y).numpy()))\n",
    "            \n",
    "fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2.5506436e-06 8.7754463e-04 9.9911994e-01]\n",
      " [1.0952300e-03 9.1185570e-02 9.0771919e-01]\n",
      " [1.1513348e-07 1.7789358e-01 8.2210630e-01]\n",
      " [1.5890674e-06 8.3665663e-01 1.6334181e-01]\n",
      " [2.7380353e-01 7.1308005e-01 1.3116424e-02]\n",
      " [1.4408457e-01 8.5591483e-01 5.6868009e-07]\n",
      " [7.3764908e-01 2.6234201e-01 8.8387587e-06]\n",
      " [9.1215891e-01 8.7841049e-02 1.2655407e-07]], shape=(8, 3), dtype=float32)\n",
      "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n",
      "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "a = hypothesis(x_data)\n",
    "print(a)\n",
    "print(tf.argmax(a, 1))\n",
    "print(tf.argmax(y_data, 1)) # matches with y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) (101, 7)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.int32) #tf1.13.1에서는 np.int32, 이전에는 np.float32\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "nb_classes = 7  # 0 ~ 6\n",
    "\n",
    "# Make Y data as onehot shape\n",
    "Y_one_hot = tf.one_hot(list(y_data), nb_classes)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "\n",
    "print(x_data.shape, Y_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weight and bias setting\n",
    "W = tf.Variable(tf.random.normal([16, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([nb_classes]), name='bias')\n",
    "variables = [W, b]\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "def logit_fn(X):\n",
    "    return tf.matmul(tf.dtypes.cast(X,tf.float32), W) + b\n",
    "\n",
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(logit_fn(X))\n",
    "\n",
    "def cost_fn(X, Y):\n",
    "    logits = logit_fn(X)\n",
    "    cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                           labels=Y)\n",
    "    cost = tf.reduce_mean(cost_i)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def grad_fn(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = cost_fn(X, Y)\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        return grads\n",
    "    \n",
    "def prediction(X, Y):\n",
    "    pred = tf.argmax(hypothesis(X), 1)\n",
    "    correct_prediction = tf.equal(pred, tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1 Loss: 0.09754873067140579, Acc: 0.9900990128517151\n",
      "Steps: 100 Loss: 0.08912097662687302, Acc: 0.9900990128517151\n",
      "Steps: 200 Loss: 0.08197152614593506, Acc: 1.0\n",
      "Steps: 300 Loss: 0.07589178532361984, Acc: 1.0\n",
      "Steps: 400 Loss: 0.07066173851490021, Acc: 1.0\n",
      "Steps: 500 Loss: 0.06611780822277069, Acc: 1.0\n",
      "Steps: 600 Loss: 0.06213541328907013, Acc: 1.0\n",
      "Steps: 700 Loss: 0.05861821398139, Acc: 1.0\n",
      "Steps: 800 Loss: 0.055490314960479736, Acc: 1.0\n",
      "Steps: 900 Loss: 0.052691176533699036, Acc: 1.0\n",
      "Steps: 1000 Loss: 0.05017192289233208, Acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "def fit(X, Y, epochs=1000, verbose=100):\n",
    "    optimizer =  tf.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        grads = grad_fn(X, Y)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        if (i==0) | ((i+1)%verbose==0):\n",
    "            # print('Loss at epoch %d: %f' %(i+1, cost_fn(X, Y).numpy()))\n",
    "            acc = prediction(X, Y).numpy()\n",
    "            loss = cost_fn(X, Y).numpy() \n",
    "            print('Steps: {} Loss: {}, Acc: {}'.format(i+1, loss, acc))\n",
    "\n",
    "fit(x_data, Y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
