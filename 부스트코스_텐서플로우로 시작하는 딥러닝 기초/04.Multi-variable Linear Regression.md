## Multi-variable Linear Regression

**Hypothesis**

H(x1, x2, x3) = w1x1 + w2x2 + w3x3 + b



**Hypothesis using matrix**

변수의 개수가 정해지지 않았기 때문에 matrix(행렬곱)으로 Hypothesis를 나타낼 수 있다.

​																	`H(X) = XW`

행렬 곱을 실행하기 때문에 행과 열의 개수를 맞추기 위해 `WX` 가 아닌 `XW` 를 쓴다.

```python
# data and label
x1 = [ 73.,  93.,  89.,  96.,  73.]
x2 = [ 80.,  88.,  91.,  98.,  66.]
x3 = [ 75.,  93.,  90., 100.,  70.]
Y  = [152., 185., 180., 196., 142.]

# random weights
w1 = tf.Variable(tf.random.normal([1]))
w2 = tf.Variable(tf.random.normal([1]))
w3 = tf.Variable(tf.random.normal([1]))
b  = tf.Variable(tf.random.normal([1]))

learning_rate = 0.000001

for i in range(1000+1):
    # tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape:
        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b
        cost = tf.reduce_mean(tf.square(hypothesis - Y))
    # calculates the gradients of the cost
    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])
    
    # update w1,w2,w3 and b
    w1.assign_sub(learning_rate * w1_grad)
    w2.assign_sub(learning_rate * w2_grad)
    w3.assign_sub(learning_rate * w3_grad)
    b.assign_sub(learning_rate * b_grad)

    if i % 50 == 0:
      print("{:5} | {:12.4f}".format(i, cost.numpy()))

'''
    0 |    7575.0693
   50 |     140.8121
  100 |      58.1745
  150 |      57.1111
  200 |      56.9531
  250 |      56.8058
  300 |      56.6587
  350 |      56.5122
  400 |      56.3662
  450 |      56.2205
  500 |      56.0753
  550 |      55.9304
  600 |      55.7858
  650 |      55.6417
  700 |      55.4979
  750 |      55.3544
  800 |      55.2114
  850 |      55.0688
  900 |      54.9265
  950 |      54.7847
 1000 |      54.6432
 '''
```

**Matrix 사용**

```python
data = np.array([
    # X1,   X2,    X3,   y
    [ 73.,  80.,  75., 152. ],
    [ 93.,  88.,  93., 185. ],
    [ 89.,  91.,  90., 180. ],
    [ 96.,  98., 100., 196. ],
    [ 73.,  66.,  70., 142. ]
], dtype=np.float32)

# slice data
X = data[:, :-1]	# data 3열까지(입력)
y = data[:, [-1]]	# data 4열(출력)

W = tf.Variable(tf.random.normal([3, 1]))	# 3행 1열
b = tf.Variable(tf.random.normal([1]))

learning_rate = 0.000001

# hypothesis, prediction function
def predict(X):
    return tf.matmul(X, W) + b	# matmul => matrix multiply(행렬곱)

print("epoch | cost")

n_epochs = 2000
for i in range(n_epochs+1):
    # tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape:
        cost = tf.reduce_mean((tf.square(predict(X) - y)))

    # calculates the gradients of the loss
    W_grad, b_grad = tape.gradient(cost, [W, b])

    # updates parameters (W and b)
    W.assign_sub(learning_rate * W_grad)
    b.assign_sub(learning_rate * b_grad)
    
    if i % 100 == 0:
        print("{:5} | {:10.4f}".format(i, cost.numpy()))
        
'''
epoch | cost
    0 |  4567.1289
  100 |     2.3131
  200 |     1.7445
  300 |     1.7378
  400 |     1.7313
  500 |     1.7249
  600 |     1.7184
  700 |     1.7121
  800 |     1.7057
  900 |     1.6994
 1000 |     1.6931
 1100 |     1.6868
 1200 |     1.6805
 1300 |     1.6743
 1400 |     1.6681
 1500 |     1.6621
 1600 |     1.6560
 1700 |     1.6499
 1800 |     1.6438
 1900 |     1.6378
 2000 |     1.6319
 '''
```

**Matrix를 사용했을 때가 더 효율적이라는 것을 알 수 있다.**

