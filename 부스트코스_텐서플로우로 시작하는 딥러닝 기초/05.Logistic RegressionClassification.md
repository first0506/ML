## Logistic Regression/Classification

**Classification**

variable is either 0 or 1 (0:positive / 1:negative)

* Exam : Pass or Fail
* Spam : Not Spam or Spam
* Face : Real or Fake
* Tumor : Not Malignant or Malignant

To start with machine learning, you must encode variable [0, 1]

```python
x_train = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]
y_train = [[0], [0], [0], [1], [1], [1]]
```



**Logistic vs Linear**

Logistic

* Discrete(Counted) - ex) Shoe Size / The number of workers in a company

Linear

* Continuous(Measured) - Time / Weight / Height

```python
Logistic_Y = [[0], [0], [0], [1], [1], [1]]	# One hot
Linear_Y = [[864.6584], [684.6843], [654.4655]] # Numeric
```



**Sigmoid(Logistic) function**

g(z) function out value is between 0 and 1

`X ---> Linear function ---> Logistic function ---> Decision Boundary ---> Y (0, 1)`

Where we define g(z) -> z is a real number -> g(z) = e^z / (e^z + 1) = 1 / (1+e^-z)

```python
hypothesis = th.sigmoid(Z)	# z = tf.matmul(X, a) + b
hypothesis = tf.div(1., 1. + tf.exp(z))
```



**Decision Boundary**

Linear / Non-linear decision boundary

if z is positive, g(z) is greater than 0.5 (0.5가 기준)

```python
predicted = tf.cast(hypothesis > 0.5, dtype=tf.int32)
```



**Cost Function**

the cost function to fit the parameters(W)

```python
def loss_fn(hypothesis, labels):
    cost = -tf.reduce_mean(labels * tf.log(hypothesis) + (1 - labels) * tf.log(1 - hypothesis))
    return cost
```



**Optimization**

How to minimize the cost function

```python
def grad(hypothesis, labels):
    with tf.GradientTape() as tape:
        loss_value = loss_fn(hypothesis, labels)
    return tape.gradient(loss_value, [W, b])
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
optimizer.apply_gradients(grads_and_vars=zip(grads,[W, b]))
```



**Code(Eager)**

```python
x_train = [[1., 2.],
          [2., 3.],
          [3., 1.],
          [4., 3.],
          [5., 3.],
          [6., 2.]]
y_train = [[0.],
          [0.],
          [0.],
          [1.],
          [1.],
          [1.]]

x_test = [[5.,2.]]	#데이터셋 설정
y_test = [[1.]]

# Batch Size는 한 번에 학습시킬 Size로 정한다.
dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))
W = tf.Variable(tf.zeros([2,1]), name='weight')
b = tf.Variable(tf.zeros([1]), name='bias')	# W, b 초기설정

def logistic_regression(features):	# Sigmoid 함수를 가설로 선언
    hypothesis  = tf.divide(1., 1. + tf.exp(tf.matmul(features, W) + b))
    return hypothesis

def loss_fn(hypothesis, features, labels):	# 가설을 검증할 Cost 함수 정의
    cost = -tf.reduce_mean(labels * tf.math.log(logistic_regression(features)) + (1 - labels) * tf.math.log(1 - hypothesis))
    return cost
optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)

def accuracy_fn(hypothesis, labels):	# 예측값이 0.5보다 크면 1 반환, 아니면 0 반환
    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))
    return accuracy
# tf.cast
# 텐서를 새로운 형태로 캐스팅하는데 사용
# 부동소수점형에서 정수형으로 바꾼 경우 소수점 버림
# Boolean 형태인 경우 True이면 1, False이면 0을 출력한다. 

def grad(features, labels):	# 경사값 계산
    with tf.GradientTape() as tape:
        loss_value = loss_fn(logistic_regression(features),features,labels)
    return tape.gradient(loss_value, [W,b])

EPOCHS = 1001

for step in range(EPOCHS):
    for features, labels  in iter(dataset):
        grads = grad(features, labels)
        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))
        if step % 100 == 0:
            print("Iter: {}, Loss: {:.4f}".format(step, loss_fn(logistic_regression(features),features,labels)))
test_acc = accuracy_fn(logistic_regression(x_test),y_test)
print("Testset Accuracy: {:.4f}".format(test_acc))
'''
Iter: 0, Loss: 0.6874
Iter: 100, Loss: 0.5776
Iter: 200, Loss: 0.5349
Iter: 300, Loss: 0.5054
Iter: 400, Loss: 0.4838
Iter: 500, Loss: 0.4671
Iter: 600, Loss: 0.4535
Iter: 700, Loss: 0.4420
Iter: 800, Loss: 0.4319
Iter: 900, Loss: 0.4228
Iter: 1000, Loss: 0.4144
Testset Accuracy: 1.0000
'''
```

